# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZUXS8-L45hT6CBGBTWVH_7lU2zremCqi
"""

# Install XGBoost if not already installed
!pip install xgboost

# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score
import xgboost as xgb

# prompt: read data from TRIAL.xlsx

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score
import xgboost as xgb
# Install XGBoost if not already installed
!pip install xgboost

# Assuming TRIAL.xlsx is in your current working directory or you provide the full path
try:
  df = pd.read_excel('TRIAL.xlsx')
  print("Data loaded successfully.")
except FileNotFoundError:
  print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
except Exception as e:
  print(f"An error occurred: {e}")

# prompt: use XGM method to build a model to predict shrinkage strain, features are w/c, a/c, dt, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the mode

from sklearn.metrics import mean_absolute_error, r2_score

# Assuming 'df' is already loaded as in your previous code

# Define features (X) and target variable (y)
X = df[['w/c', 'a/c', 'dt']]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the XGBoost model
model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42) # Specify objective
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")

# prompt: use GridSearchCV to find the best hyperparameters

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, accuracy_score
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, r2_score

# Install XGBoost if not already installed
!pip install xgboost

# Assuming TRIAL.xlsx is in your current working directory or you provide the full path
try:
  df = pd.read_excel('TRIAL.xlsx')
  print("Data loaded successfully.")
except FileNotFoundError:
  print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
except Exception as e:
  print(f"An error occurred: {e}")


# Assuming 'df' is already loaded

# Define features (X) and target variable (y)
X = df[['w/c', 'a/c', 'dt']]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
}

# Initialize the XGBoost model
model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"Best Hyperparameters: {best_params}")

# Make predictions on the test set using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")

# prompt: Visualize Feature Importance

import matplotlib.pyplot as plt

# Get feature importances from the best model
feature_importances = best_model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

# prompt: plot evaluation of predicted and actual test result

import matplotlib.pyplot as plt

# Assuming y_test and y_pred are already defined from previous code

# Create the plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Add diagonal line
plt.grid(True)
plt.show()

# prompt: use SHAP to conduct sensitivity analysis

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split, GridSearchCV
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install shap

# ... (Your existing code for data loading, model training, and hyperparameter tuning) ...

# Assuming best_model is your trained XGBoost model and X_test is your test data

# Explain the model's predictions using SHAP values
explainer = shap.Explainer(best_model)
shap_values = explainer(X_test)

# Summary plot
shap.plots.beeswarm(shap_values)

# Dependence plot (example with 'w/c' feature)
shap.dependence_plot("w/c", shap_values.values, X_test)

# Force plot (example for the first instance in the test set)
shap.force_plot(explainer.expected_value, shap_values.values[0,:], X_test.iloc[0,:])

# prompt: use XGM method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, SiO2, FlyAsh, Filler/c, Super (l), AEA (l), r, b, h, V/S, H0, t dry, T, RH_test,  use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the mode

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install xgboost shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit() # Exit if file not found
except Exception as e:
    print(f"An error occurred: {e}")
    exit()


# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'SiO2', 'FlyAsh', 'Filler/c', 'Super (l)', 'AEA (l)', 'r', 'b', 'h', 'V/S', 'H0', 't dry', 'T', 'RH_test']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
}

# Initialize the XGBoost model
model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"Best Hyperparameters: {best_params}")

# Make predictions on the test set using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")

# ... (rest of your code for visualizations)



# prompt: Visualize Feature Importance

# Get feature importances from the best model
feature_importances = best_model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

# prompt: plot evaluation of predicted and actual test result

import matplotlib.pyplot as plt

# Assuming y_test and y_pred are already defined from previous code

# Create the plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Add diagonal line
plt.grid(True)
plt.show()

# prompt: use SHAP to conduct sensitivity analysis

# Install necessary libraries
!pip install shap

# ... (Your existing code for data loading, model training, and hyperparameter tuning) ...

# Assuming best_model is your trained XGBoost model and X_test is your test data

# Explain the model's predictions using SHAP values
explainer = shap.Explainer(best_model)
shap_values = explainer(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test)

# Dependence plot (example with 'w/c' feature)
shap.dependence_plot("w/c", shap_values.values, X_test)

# Force plot (example for the first instance in the test set)
shap.force_plot(explainer.expected_value, shap_values.values[0,:], X_test.iloc[0,:])

# prompt: use XGM method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, t dry, T, RH_test,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the mode

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install xgboost shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit() # Exit if file not found
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 't dry', 'T', 'RH_test']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
}

# Initialize the XGBoost model
model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Initialize GridSearchCV (using 'neg_mean_squared_error' as the scoring metric)
grid_search = xgb.XGBClassifier()
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)


# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"Best Hyperparameters: {best_params}")

# Make predictions on the test set using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")

# ... (rest of your code for visualizations) ...

# prompt: visulize the importance of all features

import matplotlib.pyplot as plt
import pandas as pd
# ... (Your existing code) ...

# Get feature importances from the best model
feature_importances = best_model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

# prompt: plot evaluation of predicted and actual test result

import matplotlib.pyplot as plt

# Assuming y_test and y_pred are already defined from previous code

# Create the plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Add diagonal line
plt.grid(True)
plt.show()

# prompt: conduct SHAP for sensitivity analysis

# Install necessary libraries
!pip install shap

import shap

# ... (Your existing code for data loading, model training, and hyperparameter tuning) ...

# Assuming best_model is your trained XGBoost model and X_test is your test data

# Explain the model's predictions using SHAP values
explainer = shap.Explainer(best_model)
shap_values = explainer(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test)

# Dependence plot (example with 'w/c' feature)
shap.dependence_plot("w/c", shap_values.values, X_test)

# Force plot (example for the first instance in the test set)
shap.force_plot(explainer.expected_value, shap_values.values[0,:], X_test.iloc[0,:])

# prompt: use XGM method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install xgboost shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()  # Exit if file not found
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']  # Use specified features
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
}

# Initialize the XGBoost model
model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"Best Hyperparameters: {best_params}")

# Make predictions on the test set using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")

# ... (rest of your code for visualizations) ...

# Feature Importance Plot
feature_importances = best_model.feature_importances_
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# SHAP Value Plots
explainer = shap.Explainer(best_model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)
shap.dependence_plot("w/c", shap_values.values, X_test)
shap.force_plot(explainer.expected_value, shap_values.values[0,:], X_test.iloc[0,:])

# prompt: use SVM method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# ... (Your existing code for data loading and preprocessing) ...

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']  # Use specified features
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the SVR model
model = SVR() # You can add parameters like kernel, C, epsilon etc here for optimization
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")

# ... (rest of your code for visualizations) ...

# prompt: use GridSearchCV to find the best hyperparameters

# ... (Your existing code) ...

# Define the parameter grid for GridSearchCV with SVR
param_grid_svr = {
    'kernel': ['linear', 'rbf', 'poly'],
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto', 0.1, 1],
    'epsilon': [0.01, 0.1, 1]
}

# Initialize the SVR model
svr_model = SVR()

# Initialize GridSearchCV for SVR
grid_search_svr = GridSearchCV(estimator=svr_model, param_grid=param_grid_svr, scoring='neg_mean_squared_error', cv=3, verbose=1)

# Fit GridSearchCV to the training data
grid_search_svr.fit(X_train, y_train)

# Get the best SVR model and its hyperparameters
best_svr_model = grid_search_svr.best_estimator_
best_svr_params = grid_search_svr.best_params_

print(f"Best SVR Hyperparameters: {best_svr_params}")

# Make predictions using the best SVR model
y_pred_svr = best_svr_model.predict(X_test)

# Evaluate the best SVR model
rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))
mae_svr = mean_absolute_error(y_test, y_pred_svr)
r2_svr = r2_score(y_test, y_pred_svr)

print(f"SVR RMSE: {rmse_svr}")
print(f"SVR MAE: {mae_svr}")
print(f"SVR R-squared: {r2_svr}")

# ... (rest of your code for visualizations, using y_pred_svr)

# prompt: use SVM method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, use GridSearchCV to find the best hyperparameters

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'kernel': ['linear', 'rbf', 'poly'],
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto', 0.1, 1],
    'epsilon': [0.01, 0.1, 1]
}

# Initialize the SVR model
svr_model = SVR()

# Initialize GridSearchCV for SVR
grid_search_svr = GridSearchCV(estimator=svr_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)

# Fit GridSearchCV to the training data
grid_search_svr.fit(X_train, y_train)

# Get the best SVR model and its hyperparameters
best_svr_model = grid_search_svr.best_estimator_
best_svr_params = grid_search_svr.best_params_

print(f"Best SVR Hyperparameters: {best_svr_params}")

# Make predictions using the best SVR model
y_pred = best_svr_model.predict(X_test)

# Evaluate the best SVR model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"SVR RMSE: {rmse}")
print(f"SVR MAE: {mae}")
print(f"SVR R-squared: {r2}")

#Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

#SHAP Value Plots
explainer = shap.Explainer(best_svr_model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

# prompt: use Random forest method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, use GridSearchCV to find the best hyperparameters

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the RandomForestRegressor model
rf_model = RandomForestRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_rf_model = grid_search.best_estimator_
best_rf_params = grid_search.best_params_

print(f"Best Random Forest Hyperparameters: {best_rf_params}")

# Make predictions using the best model
y_pred = best_rf_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Random Forest RMSE: {rmse}")
print(f"Random Forest MAE: {mae}")
print(f"Random Forest R-squared: {r2}")

# Feature Importance Plot
feature_importances = best_rf_model.feature_importances_
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# SHAP Value Plots
explainer = shap.Explainer(best_rf_model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)
shap.dependence_plot("w/c", shap_values.values, X_test)
shap.force_plot(explainer.expected_value, shap_values.values[0,:], X_test.iloc[0,:])

# prompt: use decision tree method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, use GridSearchCV to find the best hyperparameters

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the DecisionTreeRegressor model
dt_model = DecisionTreeRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_dt_model = grid_search.best_estimator_
best_dt_params = grid_search.best_params_

print(f"Best Decision Tree Hyperparameters: {best_dt_params}")

# Make predictions using the best model
y_pred = best_dt_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Decision Tree RMSE: {rmse}")
print(f"Decision Tree MAE: {mae}")
print(f"Decision Tree R-squared: {r2}")

# Feature Importance Plot
feature_importances = best_dt_model.feature_importances_
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# SHAP Value Plots
explainer = shap.Explainer(best_dt_model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)
shap.dependence_plot("w/c", shap_values.values, X_test)
shap.force_plot(explainer.expected_value, shap_values.values[0,:], X_test.iloc[0,:])

# prompt: use adaboost method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, use GridSearchCV to find the best hyperparameters

!pip install pandas numpy scikit-learn xgboost matplotlib shap

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1],
    'loss': ['linear', 'square', 'exponential']
}

# Initialize the AdaBoostRegressor model
ada_model = AdaBoostRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=ada_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_ada_model = grid_search.best_estimator_
best_ada_params = grid_search.best_params_

print(f"Best AdaBoost Hyperparameters: {best_ada_params}")

# Make predictions using the best model
y_pred = best_ada_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"AdaBoost RMSE: {rmse}")
print(f"AdaBoost MAE: {mae}")
print(f"AdaBoost R-squared: {r2}")

# Feature Importance (if available for the model)
# Note: Feature importance might not be directly available for all regressors
try:
    feature_importances = best_ada_model.feature_importances_
    importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
    importance_df = importance_df.sort_values(by='Importance', ascending=False)
    plt.figure(figsize=(10, 6))
    plt.bar(importance_df['Feature'], importance_df['Importance'])
    plt.xlabel('Feature')
    plt.ylabel('Importance')
    plt.title('Feature Importance')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
except AttributeError:
    print("Feature importances not available for this model.")

# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# SHAP Value Plots
# Instead of using algorithm='tree', use the model directly
explainer = shap.TreeExplainer(best_ada_model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)
shap.dependence_plot("w/c", shap_values.values, X_test)
shap.force_plot(explainer.expected_value, shap_values.values[0,:], X_test.iloc[0,:])

# prompt: use KNN method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, use GridSearchCV to find the best hyperparameters, plot feature importance, predicted value and actual value, and use SHAP to conducted sensitivity analysis

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsRegressor
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

# Initialize the KNN model
knn_model = KNeighborsRegressor()

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_knn_model = grid_search.best_estimator_
best_knn_params = grid_search.best_params_

print(f"Best KNN Hyperparameters: {best_knn_params}")

# Make predictions using the best model
y_pred = best_knn_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"KNN RMSE: {rmse}")
print(f"KNN MAE: {mae}")
print(f"KNN R-squared: {r2}")


# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# SHAP Value Plots
# Use KernelExplainer for KNN models
explainer = shap.KernelExplainer(best_knn_model.predict, X_train) # Use X_train for background data
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=features) # Add feature_names
shap.dependence_plot("w/c", shap_values, X_test, feature_names=features) # Add feature_names

#You can also try force plot for individual predictions
#shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])

# prompt: visulize the importance

# Assuming 'best_rf_model' is your trained RandomForestRegressor model and 'X_test' contains your test features

import matplotlib.pyplot as plt
import shap

# ... (Your existing code) ...

# Feature Importance Plot (using the best model from GridSearchCV)
feature_importances = best_rf_model.feature_importances_
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()


# SHAP Value Plots
explainer = shap.Explainer(best_rf_model) # Use your best model here
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

#You can also try dependence plots and force plots:
shap.dependence_plot("w/c", shap_values.values, X_test)
shap.force_plot(explainer.expected_value, shap_values.values[0,:], X_test.iloc[0,:])

# prompt: use deep CNN method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, use find the best hyperparameters, plot feature importance, predicted value and actual value, and use SHAP to conducted sensitivity analysis

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import shap
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

!pip install shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the deep CNN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(len(features),)),
    layers.Dense(128, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'epochs': [50, 100, 150],
    'batch_size': [32, 64, 128]
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=keras.wrappers.scikit_learn.KerasRegressor(build_fn=lambda: model),
                           param_grid=param_grid,
                           scoring='neg_mean_squared_error',
                           cv=3,
                           verbose=1)

grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_model = grid_search.best_estimator_.model
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Make predictions
y_pred = best_model.predict(X_test)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")


# Feature Importance (SHAP values for deep learning models)
# Requires a workaround because SHAP doesn't directly support Keras models
# We can use KernelExplainer with a prediction function
explainer = shap.KernelExplainer(best_model.predict, X_train)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=features)

# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# SHAP dependence plot
shap.dependence_plot("w/c", shap_values, X_test, feature_names=features)

# prompt: use deep CNN method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, find the best hyperparameters, plot feature importance, predicted value and actual value, and use SHAP to conducted sensitivity analysis

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import shap
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import GridSearchCV

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the deep CNN model
def create_model():
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(len(features),)),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(1)  # Output layer for regression
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Use KerasRegressor for GridSearchCV compatibility
model = keras.wrappers.scikit_learn.KerasRegressor(build_fn=create_model, verbose=0)


# Define the parameter grid for hyperparameter tuning
param_grid = {
    'epochs': [50, 100],
    'batch_size': [32, 64],
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_model = grid_search.best_estimator_.model
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Make predictions
y_pred = best_model.predict(X_test)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")

# Feature Importance (SHAP values for deep learning models)
explainer = shap.KernelExplainer(best_model.predict, X_train)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=features)

# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# SHAP dependence plot
shap.dependence_plot("w/c", shap_values, X_test, feature_names=features)

# prompt: use GB method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, find the best hyperparameters, plot feature importance, predicted value and actual value, and use SHAP to conducted sensitivity analysis

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Gradient Boosting Regressor model
gb_model = GradientBoostingRegressor(random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.5],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its hyperparameters
best_gb_model = grid_search.best_estimator_
best_gb_params = grid_search.best_params_

print(f"Best Gradient Boosting Hyperparameters: {best_gb_params}")

# Make predictions using the best model
y_pred = best_gb_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Gradient Boosting RMSE: {rmse}")
print(f"Gradient Boosting MAE: {mae}")
print(f"Gradient Boosting R-squared: {r2}")

# Feature Importance Plot
feature_importances = best_gb_model.feature_importances_
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# SHAP Value Plots
explainer = shap.TreeExplainer(best_gb_model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=features)
shap.dependence_plot("w/c", shap_values, X_test, feature_names=features)

# prompt: use CNN method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, find the best hyperparameters, plot feature importance, predicted value and actual value, and use SHAP to conducted sensitivity analysis

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import shap
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import GridSearchCV

# Install necessary libraries
!pip install shap tensorflow

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the CNN model
def create_cnn_model():
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(len(features),)),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(1)  # Output layer for regression
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Create the KerasRegressor wrapper
cnn_model = keras.wrappers.scikit_learn.KerasRegressor(build_fn=create_cnn_model, verbose=0)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'epochs': [50, 100],
    'batch_size': [32, 64],
}

# Perform hyperparameter tuning using GridSearchCV
grid_search = GridSearchCV(estimator=cnn_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)
grid_search.fit(X_train, y_train)

# Get the best model and hyperparameters
best_cnn_model = grid_search.best_estimator_.model
best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Make predictions
y_pred = best_cnn_model.predict(X_test)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R-squared: {r2}")

# SHAP values for feature importance and sensitivity analysis
explainer = shap.KernelExplainer(best_cnn_model.predict, X_train)
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test, feature_names=features)

# Dependence plot for 'w/c'
shap.dependence_plot("w/c", shap_values, X_test, feature_names=features)

# Predicted vs Actual plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()



# prompt: plot corelation for , features are w/c, a/c, dt, c, H0, T,  and target shrinkage strain , drop other unused columns,

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'X' and 'y' are defined as in your previous code
correlation_matrix = pd.concat([X, y], axis=1).corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features and Shrinkage Strain')
plt.show()

# prompt: plot how the prediction is made

# ... (Your existing code) ...

# Assuming 'best_knn_model' is your trained KNN model and 'X_test' contains your test features
# Create a sample data point for prediction demonstration.  Use an actual data point from X_test for a real example.
sample_data_point = X_test.iloc[0] # Example: Using the first data point in X_test

# Make a prediction for the sample point
predicted_value = best_knn_model.predict(pd.DataFrame([sample_data_point]))  # Predict

# Display the sample point and the prediction
print("Sample Data Point:")
print(sample_data_point)
print("\nPredicted Value:", predicted_value[0]) # Accessing the first (and only) element of the prediction array

#Now, explain the prediction with SHAP values
explainer = shap.KernelExplainer(best_knn_model.predict, X_train)
shap_values_sample = explainer.shap_values(pd.DataFrame([sample_data_point]))

# Force plot for individual predictions
shap.force_plot(explainer.expected_value, shap_values_sample[0,:], sample_data_point)

# ... (rest of your code)

# prompt: use XGB method to build a model to predict shrinkage strain, features are w/c, a/c, dt, c, H0, T,  drop other unused columns, use 80% to train, 20% to verify, use RMSE, MAE, coefficient of determination to evaluate the model, plot how the model is trained

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
import shap

# Install necessary libraries
!pip install shap

# Load the dataset
try:
    df = pd.read_excel('TRIAL.xlsx')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: 'TRIAL.xlsx' not found. Please make sure the file exists in the current directory or provide the correct path.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    exit()

# Define features (X) and target variable (y)
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']
X = df[features]
y = df['shrinkage strain']

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.5],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Perform GridSearchCV for hyperparameter tuning
from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1)
grid_search.fit(X_train, y_train)


# Get the best model and its hyperparameters
best_xgb_model = grid_search.best_estimator_
best_xgb_params = grid_search.best_params_
print(f"Best XGBoost Hyperparameters: {best_xgb_params}")


# Make predictions using the best model
y_pred = best_xgb_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"XGBoost RMSE: {rmse}")
print(f"XGBoost MAE: {mae}")
print(f"XGBoost R-squared: {r2}")

# Feature Importance Plot
feature_importances = best_xgb_model.feature_importances_
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Plot training progress (you'll need to modify this based on your actual training history)
# Example: assuming you have a 'results' dictionary from training
# results = {'train-rmse-mean': train_rmse_mean, 'test-rmse-mean':test_rmse_mean }
# epochs = range(1, len(results['train-rmse-mean']) + 1)
# plt.plot(epochs, results['train-rmse-mean'], label='Training RMSE')
# plt.plot(epochs, results['test-rmse-mean'], label='Test RMSE')
# plt.xlabel('Epochs')
# plt.ylabel('RMSE')
# plt.legend()
# plt.show()

# SHAP Value Plots
explainer = shap.TreeExplainer(best_xgb_model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=features)
shap.dependence_plot("w/c", shap_values, X_test, feature_names=features)

# Predicted vs Actual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Test Results")
plt.ylabel("Predicted Test Results")
plt.title("Evaluation of Predicted vs. Actual Test Results")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.grid(True)
plt.show()

# prompt: plot a pie chat to show the contribution of each feature

import matplotlib.pyplot as plt

# Assuming 'feature_importances' and 'features' are defined from the XGBoost model
feature_importances = best_xgb_model.feature_importances_
features = ['w/c', 'a/c', 'dt', 'c', 'H0', 'T']

# Create the pie chart
plt.figure(figsize=(8, 8))
plt.pie(feature_importances, labels=features, autopct='%1.1f%%', startangle=90)
plt.title('Feature Contribution to Prediction (XGBoost)')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()